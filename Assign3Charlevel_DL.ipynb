{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Assign3Charlevel_DL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEyt_fmTz3FF",
        "colab_type": "text"
      },
      "source": [
        "###Translation using Character Level Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avbh_PBFUulN",
        "colab_type": "code",
        "outputId": "c138341d-9b39-451b-ea2f-80a67df1d630",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "%tensorflow_version 1.14\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "from tensorflow import keras\n",
        "from IPython import display\n",
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.python.data import Dataset\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "print(tf.__version__)\n",
        "import string\n",
        "from string import digits\n",
        "%matplotlib inline\n",
        "import re\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Input, LSTM, Embedding, Dense\n",
        "from keras.models import Model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.14`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxvypGRUUulR",
        "colab_type": "code",
        "outputId": "ef4eaab2-c737-4363-8e18-131b687704a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!wget -N http://www.manythings.org/anki/hin-eng.zip\n",
        "!unzip -o hin-eng.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-08 02:02:11--  http://www.manythings.org/anki/hin-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.24.109.196, 104.24.108.196, 2606:4700:3037::6818:6cc4, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.24.109.196|:80... connected.\n",
            "HTTP request sent, awaiting response... 304 Not Modified\n",
            "File ‘hin-eng.zip’ not modified on server. Omitting download.\n",
            "\n",
            "Archive:  hin-eng.zip\n",
            "  inflating: hin.txt                 \n",
            "  inflating: _about.txt              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHl3IqFTUulT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 500  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "data_path = 'hin.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6cFQ5VEUulV",
        "colab_type": "code",
        "outputId": "3b6c764b-1ace-48dc-fe4f-a141d454fffa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split('\\t')\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 2778\n",
            "Number of unique input tokens: 70\n",
            "Number of unique output tokens: 92\n",
            "Max sequence length for inputs: 107\n",
            "Max sequence length for outputs: 123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFGQHLWlUulX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKdDNbg0Uulc",
        "colab_type": "code",
        "outputId": "dbec49c4-b6eb-4463-f378-99a71f067de4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URCcFJniUule",
        "colab_type": "code",
        "outputId": "2d591c32-4a15-4d74-a062-16229766b858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, None, 70)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, None, 92)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 256), (None, 334848      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, None, 256),  357376      input_2[0][0]                    \n",
            "                                                                 lstm_1[0][1]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 92)     23644       lstm_2[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 715,868\n",
            "Trainable params: 715,868\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNbiUfr4Uulg",
        "colab_type": "code",
        "outputId": "68a7f9e6-ddc0-4568-fc1e-7092d7946807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17377
        }
      },
      "source": [
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.2)\n",
        "# Save model\n",
        "model.save('s2s.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 2222 samples, validate on 556 samples\n",
            "Epoch 1/500\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "2222/2222 [==============================] - 12s 5ms/step - loss: 0.8228 - val_loss: 1.3168\n",
            "Epoch 2/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.7601 - val_loss: 1.2631\n",
            "Epoch 3/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.7254 - val_loss: 1.1910\n",
            "Epoch 4/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.6630 - val_loss: 1.0971\n",
            "Epoch 5/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.6138 - val_loss: 1.0266\n",
            "Epoch 6/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.5782 - val_loss: 0.9762\n",
            "Epoch 7/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.5476 - val_loss: 0.9382\n",
            "Epoch 8/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.5241 - val_loss: 0.9091\n",
            "Epoch 9/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.5059 - val_loss: 0.8856\n",
            "Epoch 10/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.4915 - val_loss: 0.8740\n",
            "Epoch 11/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.4789 - val_loss: 0.8823\n",
            "Epoch 12/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.4687 - val_loss: 0.8467\n",
            "Epoch 13/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.4588 - val_loss: 0.8340\n",
            "Epoch 14/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.4512 - val_loss: 0.8202\n",
            "Epoch 15/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.4520 - val_loss: 0.8144\n",
            "Epoch 16/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.4365 - val_loss: 0.7973\n",
            "Epoch 17/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.4298 - val_loss: 0.7968\n",
            "Epoch 18/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.4234 - val_loss: 0.7901\n",
            "Epoch 19/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.4169 - val_loss: 0.7935\n",
            "Epoch 20/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.4107 - val_loss: 0.7740\n",
            "Epoch 21/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.4050 - val_loss: 0.7736\n",
            "Epoch 22/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.3996 - val_loss: 0.7633\n",
            "Epoch 23/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.3933 - val_loss: 0.7594\n",
            "Epoch 24/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.3881 - val_loss: 0.7691\n",
            "Epoch 25/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.3833 - val_loss: 0.7653\n",
            "Epoch 26/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.3780 - val_loss: 0.7578\n",
            "Epoch 27/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.3726 - val_loss: 0.7534\n",
            "Epoch 28/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.3685 - val_loss: 0.7595\n",
            "Epoch 29/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.3625 - val_loss: 0.7610\n",
            "Epoch 30/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.3572 - val_loss: 0.7447\n",
            "Epoch 31/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.3529 - val_loss: 0.7550\n",
            "Epoch 32/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.3479 - val_loss: 0.7573\n",
            "Epoch 33/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.3428 - val_loss: 0.7453\n",
            "Epoch 34/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.3376 - val_loss: 0.7548\n",
            "Epoch 35/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.3329 - val_loss: 0.7666\n",
            "Epoch 36/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.3277 - val_loss: 0.7627\n",
            "Epoch 37/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.3236 - val_loss: 0.7597\n",
            "Epoch 38/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.3186 - val_loss: 0.7558\n",
            "Epoch 39/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.3138 - val_loss: 0.7605\n",
            "Epoch 40/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.3087 - val_loss: 0.7618\n",
            "Epoch 41/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.3039 - val_loss: 0.7748\n",
            "Epoch 42/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.2999 - val_loss: 0.7726\n",
            "Epoch 43/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.2946 - val_loss: 0.7732\n",
            "Epoch 44/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.2898 - val_loss: 0.7734\n",
            "Epoch 45/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.2851 - val_loss: 0.7811\n",
            "Epoch 46/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.2810 - val_loss: 0.7854\n",
            "Epoch 47/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.2762 - val_loss: 0.7956\n",
            "Epoch 48/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.2717 - val_loss: 0.7884\n",
            "Epoch 49/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.2671 - val_loss: 0.7944\n",
            "Epoch 50/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.2627 - val_loss: 0.8021\n",
            "Epoch 51/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.2588 - val_loss: 0.8132\n",
            "Epoch 52/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.2540 - val_loss: 0.8141\n",
            "Epoch 53/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.2499 - val_loss: 0.8216\n",
            "Epoch 54/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.2451 - val_loss: 0.8360\n",
            "Epoch 55/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.2405 - val_loss: 0.8480\n",
            "Epoch 56/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.2365 - val_loss: 0.8453\n",
            "Epoch 57/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.2333 - val_loss: 0.8454\n",
            "Epoch 58/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.2281 - val_loss: 0.8632\n",
            "Epoch 59/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.2251 - val_loss: 0.8616\n",
            "Epoch 60/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.2212 - val_loss: 0.8783\n",
            "Epoch 61/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.2168 - val_loss: 0.8707\n",
            "Epoch 62/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.2134 - val_loss: 0.8771\n",
            "Epoch 63/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.2105 - val_loss: 0.8863\n",
            "Epoch 64/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.2059 - val_loss: 0.9010\n",
            "Epoch 65/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.2016 - val_loss: 0.8956\n",
            "Epoch 66/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1989 - val_loss: 0.9176\n",
            "Epoch 67/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1959 - val_loss: 0.9177\n",
            "Epoch 68/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1913 - val_loss: 0.9219\n",
            "Epoch 69/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1888 - val_loss: 0.9515\n",
            "Epoch 70/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1849 - val_loss: 0.9420\n",
            "Epoch 71/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1817 - val_loss: 0.9494\n",
            "Epoch 72/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1795 - val_loss: 0.9683\n",
            "Epoch 73/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1760 - val_loss: 0.9699\n",
            "Epoch 74/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1729 - val_loss: 0.9656\n",
            "Epoch 75/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1702 - val_loss: 0.9821\n",
            "Epoch 76/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1676 - val_loss: 1.0024\n",
            "Epoch 77/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1645 - val_loss: 0.9937\n",
            "Epoch 78/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1624 - val_loss: 1.0091\n",
            "Epoch 79/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1597 - val_loss: 1.0191\n",
            "Epoch 80/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1571 - val_loss: 1.0263\n",
            "Epoch 81/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1545 - val_loss: 1.0326\n",
            "Epoch 82/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1528 - val_loss: 1.0319\n",
            "Epoch 83/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1497 - val_loss: 1.0440\n",
            "Epoch 84/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1481 - val_loss: 1.0525\n",
            "Epoch 85/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.1475 - val_loss: 1.0788\n",
            "Epoch 86/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1450 - val_loss: 1.0610\n",
            "Epoch 87/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.1489 - val_loss: 1.0643\n",
            "Epoch 88/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.1428 - val_loss: 1.0765\n",
            "Epoch 89/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1389 - val_loss: 1.0863\n",
            "Epoch 90/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1417 - val_loss: 1.0799\n",
            "Epoch 91/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1875 - val_loss: 1.0563\n",
            "Epoch 92/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1661 - val_loss: 1.0562\n",
            "Epoch 93/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.2155 - val_loss: 1.0390\n",
            "Epoch 94/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1775 - val_loss: 1.0371\n",
            "Epoch 95/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1619 - val_loss: 1.0542\n",
            "Epoch 96/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1490 - val_loss: 1.0531\n",
            "Epoch 97/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1493 - val_loss: 1.0558\n",
            "Epoch 98/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1401 - val_loss: 1.0777\n",
            "Epoch 99/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1465 - val_loss: 1.0848\n",
            "Epoch 100/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1404 - val_loss: 1.0810\n",
            "Epoch 101/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1350 - val_loss: 1.0913\n",
            "Epoch 102/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1319 - val_loss: 1.0932\n",
            "Epoch 103/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1291 - val_loss: 1.1278\n",
            "Epoch 104/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1259 - val_loss: 1.1342\n",
            "Epoch 105/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1240 - val_loss: 1.1261\n",
            "Epoch 106/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1224 - val_loss: 1.1360\n",
            "Epoch 107/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1206 - val_loss: 1.1465\n",
            "Epoch 108/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1185 - val_loss: 1.1600\n",
            "Epoch 109/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.1163 - val_loss: 1.1604\n",
            "Epoch 110/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.1153 - val_loss: 1.1659\n",
            "Epoch 111/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1132 - val_loss: 1.1684\n",
            "Epoch 112/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1121 - val_loss: 1.1858\n",
            "Epoch 113/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.1096 - val_loss: 1.1833\n",
            "Epoch 114/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1076 - val_loss: 1.1915\n",
            "Epoch 115/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1066 - val_loss: 1.2019\n",
            "Epoch 116/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.1046 - val_loss: 1.1978\n",
            "Epoch 117/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.1040 - val_loss: 1.2083\n",
            "Epoch 118/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.1030 - val_loss: 1.2184\n",
            "Epoch 119/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.1018 - val_loss: 1.2181\n",
            "Epoch 120/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.1003 - val_loss: 1.2158\n",
            "Epoch 121/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0995 - val_loss: 1.2369\n",
            "Epoch 122/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0984 - val_loss: 1.2396\n",
            "Epoch 123/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0980 - val_loss: 1.2315\n",
            "Epoch 124/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0963 - val_loss: 1.2542\n",
            "Epoch 125/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0956 - val_loss: 1.2577\n",
            "Epoch 126/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0947 - val_loss: 1.2592\n",
            "Epoch 127/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0935 - val_loss: 1.2851\n",
            "Epoch 128/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0930 - val_loss: 1.2692\n",
            "Epoch 129/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0919 - val_loss: 1.2966\n",
            "Epoch 130/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0913 - val_loss: 1.2687\n",
            "Epoch 131/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0911 - val_loss: 1.2795\n",
            "Epoch 132/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0893 - val_loss: 1.2942\n",
            "Epoch 133/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0895 - val_loss: 1.2975\n",
            "Epoch 134/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0884 - val_loss: 1.3015\n",
            "Epoch 135/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0876 - val_loss: 1.3183\n",
            "Epoch 136/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0869 - val_loss: 1.3074\n",
            "Epoch 137/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0860 - val_loss: 1.3152\n",
            "Epoch 138/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0858 - val_loss: 1.3264\n",
            "Epoch 139/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0850 - val_loss: 1.3167\n",
            "Epoch 140/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0842 - val_loss: 1.3279\n",
            "Epoch 141/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0838 - val_loss: 1.3414\n",
            "Epoch 142/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0833 - val_loss: 1.3328\n",
            "Epoch 143/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0824 - val_loss: 1.3459\n",
            "Epoch 144/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0827 - val_loss: 1.3385\n",
            "Epoch 145/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0816 - val_loss: 1.3486\n",
            "Epoch 146/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0807 - val_loss: 1.3858\n",
            "Epoch 147/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0807 - val_loss: 1.3653\n",
            "Epoch 148/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0805 - val_loss: 1.3669\n",
            "Epoch 149/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0799 - val_loss: 1.3656\n",
            "Epoch 150/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0792 - val_loss: 1.3697\n",
            "Epoch 151/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0785 - val_loss: 1.3782\n",
            "Epoch 152/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0784 - val_loss: 1.3860\n",
            "Epoch 153/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0786 - val_loss: 1.3824\n",
            "Epoch 154/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0775 - val_loss: 1.3861\n",
            "Epoch 155/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0772 - val_loss: 1.3884\n",
            "Epoch 156/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0772 - val_loss: 1.4003\n",
            "Epoch 157/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0759 - val_loss: 1.4054\n",
            "Epoch 158/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0761 - val_loss: 1.3958\n",
            "Epoch 159/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0751 - val_loss: 1.4089\n",
            "Epoch 160/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0751 - val_loss: 1.4199\n",
            "Epoch 161/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0743 - val_loss: 1.4013\n",
            "Epoch 162/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0742 - val_loss: 1.4157\n",
            "Epoch 163/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0734 - val_loss: 1.4145\n",
            "Epoch 164/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0738 - val_loss: 1.4197\n",
            "Epoch 165/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0729 - val_loss: 1.4256\n",
            "Epoch 166/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0727 - val_loss: 1.4342\n",
            "Epoch 167/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0727 - val_loss: 1.4284\n",
            "Epoch 168/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0717 - val_loss: 1.4234\n",
            "Epoch 169/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0721 - val_loss: 1.4276\n",
            "Epoch 170/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0719 - val_loss: 1.4289\n",
            "Epoch 171/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0717 - val_loss: 1.4356\n",
            "Epoch 172/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0716 - val_loss: 1.4344\n",
            "Epoch 173/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0713 - val_loss: 1.4444\n",
            "Epoch 174/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0708 - val_loss: 1.4584\n",
            "Epoch 175/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0702 - val_loss: 1.4421\n",
            "Epoch 176/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0704 - val_loss: 1.4375\n",
            "Epoch 177/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0695 - val_loss: 1.4390\n",
            "Epoch 178/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0692 - val_loss: 1.4593\n",
            "Epoch 179/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0700 - val_loss: 1.4566\n",
            "Epoch 180/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0695 - val_loss: 1.4556\n",
            "Epoch 181/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0691 - val_loss: 1.4579\n",
            "Epoch 182/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0685 - val_loss: 1.4521\n",
            "Epoch 183/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0683 - val_loss: 1.4676\n",
            "Epoch 184/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0678 - val_loss: 1.4722\n",
            "Epoch 185/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0674 - val_loss: 1.4868\n",
            "Epoch 186/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0676 - val_loss: 1.4605\n",
            "Epoch 187/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0673 - val_loss: 1.4642\n",
            "Epoch 188/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0667 - val_loss: 1.4822\n",
            "Epoch 189/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0667 - val_loss: 1.4807\n",
            "Epoch 190/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0662 - val_loss: 1.4947\n",
            "Epoch 191/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0662 - val_loss: 1.4897\n",
            "Epoch 192/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0658 - val_loss: 1.4928\n",
            "Epoch 193/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0670 - val_loss: 1.4907\n",
            "Epoch 194/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0666 - val_loss: 1.5132\n",
            "Epoch 195/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0658 - val_loss: 1.4970\n",
            "Epoch 196/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0661 - val_loss: 1.5102\n",
            "Epoch 197/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0652 - val_loss: 1.4968\n",
            "Epoch 198/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0654 - val_loss: 1.5109\n",
            "Epoch 199/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0649 - val_loss: 1.5120\n",
            "Epoch 200/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0646 - val_loss: 1.5047\n",
            "Epoch 201/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0645 - val_loss: 1.5160\n",
            "Epoch 202/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0638 - val_loss: 1.5233\n",
            "Epoch 203/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0639 - val_loss: 1.5204\n",
            "Epoch 204/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0633 - val_loss: 1.5254\n",
            "Epoch 205/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0634 - val_loss: 1.5138\n",
            "Epoch 206/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0632 - val_loss: 1.5259\n",
            "Epoch 207/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0633 - val_loss: 1.5190\n",
            "Epoch 208/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0629 - val_loss: 1.5231\n",
            "Epoch 209/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0631 - val_loss: 1.5277\n",
            "Epoch 210/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0623 - val_loss: 1.5377\n",
            "Epoch 211/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0622 - val_loss: 1.5515\n",
            "Epoch 212/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0617 - val_loss: 1.5342\n",
            "Epoch 213/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0613 - val_loss: 1.5377\n",
            "Epoch 214/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0612 - val_loss: 1.5371\n",
            "Epoch 215/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0608 - val_loss: 1.5397\n",
            "Epoch 216/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0607 - val_loss: 1.5365\n",
            "Epoch 217/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0607 - val_loss: 1.5451\n",
            "Epoch 218/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0600 - val_loss: 1.5590\n",
            "Epoch 219/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0604 - val_loss: 1.5461\n",
            "Epoch 220/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0601 - val_loss: 1.5419\n",
            "Epoch 221/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0590 - val_loss: 1.5451\n",
            "Epoch 222/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0597 - val_loss: 1.5309\n",
            "Epoch 223/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0590 - val_loss: 1.5630\n",
            "Epoch 224/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0587 - val_loss: 1.5507\n",
            "Epoch 225/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0587 - val_loss: 1.5605\n",
            "Epoch 226/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0588 - val_loss: 1.5659\n",
            "Epoch 227/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0583 - val_loss: 1.5574\n",
            "Epoch 228/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0581 - val_loss: 1.5601\n",
            "Epoch 229/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0579 - val_loss: 1.5669\n",
            "Epoch 230/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0579 - val_loss: 1.5651\n",
            "Epoch 231/500\n",
            "2222/2222 [==============================] - 11s 5ms/step - loss: 0.0577 - val_loss: 1.5752\n",
            "Epoch 232/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0576 - val_loss: 1.5609\n",
            "Epoch 233/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0570 - val_loss: 1.5691\n",
            "Epoch 234/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0571 - val_loss: 1.5649\n",
            "Epoch 235/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0567 - val_loss: 1.5733\n",
            "Epoch 236/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0565 - val_loss: 1.5723\n",
            "Epoch 237/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0562 - val_loss: 1.5925\n",
            "Epoch 238/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0562 - val_loss: 1.5771\n",
            "Epoch 239/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0560 - val_loss: 1.5734\n",
            "Epoch 240/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0558 - val_loss: 1.5744\n",
            "Epoch 241/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0562 - val_loss: 1.5647\n",
            "Epoch 242/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0551 - val_loss: 1.5781\n",
            "Epoch 243/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0554 - val_loss: 1.5825\n",
            "Epoch 244/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0557 - val_loss: 1.5907\n",
            "Epoch 245/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0548 - val_loss: 1.5726\n",
            "Epoch 246/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0550 - val_loss: 1.5889\n",
            "Epoch 247/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0548 - val_loss: 1.5770\n",
            "Epoch 248/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0544 - val_loss: 1.5804\n",
            "Epoch 249/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0543 - val_loss: 1.5718\n",
            "Epoch 250/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0540 - val_loss: 1.6053\n",
            "Epoch 251/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0539 - val_loss: 1.6010\n",
            "Epoch 252/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0536 - val_loss: 1.6016\n",
            "Epoch 253/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0534 - val_loss: 1.6161\n",
            "Epoch 254/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0535 - val_loss: 1.5955\n",
            "Epoch 255/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0533 - val_loss: 1.6041\n",
            "Epoch 256/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0528 - val_loss: 1.6020\n",
            "Epoch 257/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0531 - val_loss: 1.5985\n",
            "Epoch 258/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0530 - val_loss: 1.6003\n",
            "Epoch 259/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0522 - val_loss: 1.6064\n",
            "Epoch 260/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0521 - val_loss: 1.6116\n",
            "Epoch 261/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0526 - val_loss: 1.6013\n",
            "Epoch 262/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0520 - val_loss: 1.5982\n",
            "Epoch 263/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0517 - val_loss: 1.5980\n",
            "Epoch 264/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0518 - val_loss: 1.6109\n",
            "Epoch 265/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0526 - val_loss: 1.6116\n",
            "Epoch 266/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0512 - val_loss: 1.6242\n",
            "Epoch 267/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0514 - val_loss: 1.6062\n",
            "Epoch 268/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0512 - val_loss: 1.6315\n",
            "Epoch 269/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0518 - val_loss: 1.6110\n",
            "Epoch 270/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0505 - val_loss: 1.6242\n",
            "Epoch 271/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0505 - val_loss: 1.6193\n",
            "Epoch 272/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0507 - val_loss: 1.6296\n",
            "Epoch 273/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0502 - val_loss: 1.6205\n",
            "Epoch 274/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0500 - val_loss: 1.6275\n",
            "Epoch 275/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0503 - val_loss: 1.6178\n",
            "Epoch 276/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0521 - val_loss: 1.6225\n",
            "Epoch 277/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0510 - val_loss: 1.6125\n",
            "Epoch 278/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0501 - val_loss: 1.6272\n",
            "Epoch 279/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0497 - val_loss: 1.6240\n",
            "Epoch 280/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0499 - val_loss: 1.6337\n",
            "Epoch 281/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0495 - val_loss: 1.6266\n",
            "Epoch 282/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0492 - val_loss: 1.6368\n",
            "Epoch 283/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0491 - val_loss: 1.6544\n",
            "Epoch 284/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0490 - val_loss: 1.6416\n",
            "Epoch 285/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0491 - val_loss: 1.6318\n",
            "Epoch 286/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0484 - val_loss: 1.6481\n",
            "Epoch 287/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0484 - val_loss: 1.6372\n",
            "Epoch 288/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0484 - val_loss: 1.6283\n",
            "Epoch 289/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0480 - val_loss: 1.6367\n",
            "Epoch 290/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0481 - val_loss: 1.6304\n",
            "Epoch 291/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0478 - val_loss: 1.6430\n",
            "Epoch 292/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0475 - val_loss: 1.6410\n",
            "Epoch 293/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0471 - val_loss: 1.6438\n",
            "Epoch 294/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0474 - val_loss: 1.6527\n",
            "Epoch 295/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0474 - val_loss: 1.6520\n",
            "Epoch 296/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0465 - val_loss: 1.6538\n",
            "Epoch 297/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0472 - val_loss: 1.6495\n",
            "Epoch 298/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0467 - val_loss: 1.6432\n",
            "Epoch 299/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0463 - val_loss: 1.6607\n",
            "Epoch 300/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0466 - val_loss: 1.6566\n",
            "Epoch 301/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0460 - val_loss: 1.6515\n",
            "Epoch 302/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0462 - val_loss: 1.6575\n",
            "Epoch 303/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0459 - val_loss: 1.6493\n",
            "Epoch 304/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0458 - val_loss: 1.6552\n",
            "Epoch 305/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0455 - val_loss: 1.6533\n",
            "Epoch 306/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0463 - val_loss: 1.6662\n",
            "Epoch 307/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0455 - val_loss: 1.6651\n",
            "Epoch 308/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0451 - val_loss: 1.6590\n",
            "Epoch 309/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0452 - val_loss: 1.6625\n",
            "Epoch 310/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0453 - val_loss: 1.6642\n",
            "Epoch 311/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0446 - val_loss: 1.6624\n",
            "Epoch 312/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0449 - val_loss: 1.6668\n",
            "Epoch 313/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0446 - val_loss: 1.6616\n",
            "Epoch 314/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0444 - val_loss: 1.6703\n",
            "Epoch 315/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0444 - val_loss: 1.6643\n",
            "Epoch 316/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0442 - val_loss: 1.6493\n",
            "Epoch 317/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0440 - val_loss: 1.6850\n",
            "Epoch 318/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0437 - val_loss: 1.6836\n",
            "Epoch 319/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0438 - val_loss: 1.6703\n",
            "Epoch 320/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0435 - val_loss: 1.6812\n",
            "Epoch 321/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0437 - val_loss: 1.6867\n",
            "Epoch 322/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0436 - val_loss: 1.6730\n",
            "Epoch 323/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0427 - val_loss: 1.6840\n",
            "Epoch 324/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0430 - val_loss: 1.6743\n",
            "Epoch 325/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0428 - val_loss: 1.6849\n",
            "Epoch 326/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0431 - val_loss: 1.6906\n",
            "Epoch 327/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0428 - val_loss: 1.6892\n",
            "Epoch 328/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0423 - val_loss: 1.6785\n",
            "Epoch 329/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0427 - val_loss: 1.6908\n",
            "Epoch 330/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0424 - val_loss: 1.6957\n",
            "Epoch 331/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0425 - val_loss: 1.6882\n",
            "Epoch 332/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0423 - val_loss: 1.7097\n",
            "Epoch 333/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0420 - val_loss: 1.6913\n",
            "Epoch 334/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0419 - val_loss: 1.6834\n",
            "Epoch 335/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0425 - val_loss: 1.7149\n",
            "Epoch 336/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0411 - val_loss: 1.7142\n",
            "Epoch 337/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0414 - val_loss: 1.7081\n",
            "Epoch 338/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0410 - val_loss: 1.7055\n",
            "Epoch 339/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0413 - val_loss: 1.7158\n",
            "Epoch 340/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0414 - val_loss: 1.7110\n",
            "Epoch 341/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0414 - val_loss: 1.6963\n",
            "Epoch 342/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0404 - val_loss: 1.6830\n",
            "Epoch 343/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0410 - val_loss: 1.7025\n",
            "Epoch 344/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0419 - val_loss: 1.7226\n",
            "Epoch 345/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0404 - val_loss: 1.7103\n",
            "Epoch 346/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0403 - val_loss: 1.7046\n",
            "Epoch 347/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0407 - val_loss: 1.7093\n",
            "Epoch 348/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0398 - val_loss: 1.7122\n",
            "Epoch 349/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0399 - val_loss: 1.7218\n",
            "Epoch 350/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0398 - val_loss: 1.7263\n",
            "Epoch 351/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0394 - val_loss: 1.7165\n",
            "Epoch 352/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0398 - val_loss: 1.7159\n",
            "Epoch 353/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0394 - val_loss: 1.7281\n",
            "Epoch 354/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0392 - val_loss: 1.7235\n",
            "Epoch 355/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0388 - val_loss: 1.7347\n",
            "Epoch 356/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0389 - val_loss: 1.7228\n",
            "Epoch 357/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0389 - val_loss: 1.7280\n",
            "Epoch 358/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0398 - val_loss: 1.7115\n",
            "Epoch 359/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0390 - val_loss: 1.7360\n",
            "Epoch 360/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0383 - val_loss: 1.7183\n",
            "Epoch 361/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0394 - val_loss: 1.7124\n",
            "Epoch 362/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0391 - val_loss: 1.7292\n",
            "Epoch 363/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0380 - val_loss: 1.7254\n",
            "Epoch 364/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0382 - val_loss: 1.7403\n",
            "Epoch 365/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0383 - val_loss: 1.7212\n",
            "Epoch 366/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0386 - val_loss: 1.7212\n",
            "Epoch 367/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0379 - val_loss: 1.7347\n",
            "Epoch 368/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0374 - val_loss: 1.7394\n",
            "Epoch 369/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0378 - val_loss: 1.7278\n",
            "Epoch 370/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0370 - val_loss: 1.7347\n",
            "Epoch 371/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0373 - val_loss: 1.7276\n",
            "Epoch 372/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0373 - val_loss: 1.7339\n",
            "Epoch 373/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0375 - val_loss: 1.7354\n",
            "Epoch 374/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0373 - val_loss: 1.7470\n",
            "Epoch 375/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0367 - val_loss: 1.7457\n",
            "Epoch 376/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0365 - val_loss: 1.7187\n",
            "Epoch 377/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0362 - val_loss: 1.7273\n",
            "Epoch 378/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0361 - val_loss: 1.7321\n",
            "Epoch 379/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0364 - val_loss: 1.7632\n",
            "Epoch 380/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0357 - val_loss: 1.7357\n",
            "Epoch 381/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0362 - val_loss: 1.7389\n",
            "Epoch 382/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0357 - val_loss: 1.7459\n",
            "Epoch 383/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0354 - val_loss: 1.7532\n",
            "Epoch 384/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0355 - val_loss: 1.7529\n",
            "Epoch 385/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0357 - val_loss: 1.7613\n",
            "Epoch 386/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0351 - val_loss: 1.7482\n",
            "Epoch 387/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0357 - val_loss: 1.7563\n",
            "Epoch 388/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0361 - val_loss: 1.7554\n",
            "Epoch 389/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0352 - val_loss: 1.7602\n",
            "Epoch 390/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0345 - val_loss: 1.7683\n",
            "Epoch 391/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0354 - val_loss: 1.7552\n",
            "Epoch 392/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0353 - val_loss: 1.7583\n",
            "Epoch 393/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0346 - val_loss: 1.7573\n",
            "Epoch 394/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0346 - val_loss: 1.7678\n",
            "Epoch 395/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0340 - val_loss: 1.7676\n",
            "Epoch 396/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0344 - val_loss: 1.7654\n",
            "Epoch 397/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0346 - val_loss: 1.7703\n",
            "Epoch 398/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0341 - val_loss: 1.7708\n",
            "Epoch 399/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0343 - val_loss: 1.7815\n",
            "Epoch 400/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0334 - val_loss: 1.7592\n",
            "Epoch 401/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0334 - val_loss: 1.7703\n",
            "Epoch 402/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0336 - val_loss: 1.7762\n",
            "Epoch 403/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0338 - val_loss: 1.7811\n",
            "Epoch 404/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0334 - val_loss: 1.7824\n",
            "Epoch 405/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0331 - val_loss: 1.7682\n",
            "Epoch 406/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0330 - val_loss: 1.7746\n",
            "Epoch 407/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0326 - val_loss: 1.7806\n",
            "Epoch 408/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0326 - val_loss: 1.7840\n",
            "Epoch 409/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0345 - val_loss: 1.7781\n",
            "Epoch 410/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0326 - val_loss: 1.7787\n",
            "Epoch 411/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0348 - val_loss: 1.7680\n",
            "Epoch 412/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0321 - val_loss: 1.7783\n",
            "Epoch 413/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0326 - val_loss: 1.7863\n",
            "Epoch 414/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0321 - val_loss: 1.7850\n",
            "Epoch 415/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0323 - val_loss: 1.7906\n",
            "Epoch 416/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0316 - val_loss: 1.7798\n",
            "Epoch 417/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0318 - val_loss: 1.7876\n",
            "Epoch 418/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0315 - val_loss: 1.7947\n",
            "Epoch 419/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0315 - val_loss: 1.8041\n",
            "Epoch 420/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0309 - val_loss: 1.7959\n",
            "Epoch 421/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0313 - val_loss: 1.7944\n",
            "Epoch 422/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0319 - val_loss: 1.7883\n",
            "Epoch 423/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0306 - val_loss: 1.8098\n",
            "Epoch 424/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0313 - val_loss: 1.8090\n",
            "Epoch 425/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0318 - val_loss: 1.8121\n",
            "Epoch 426/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0314 - val_loss: 1.7975\n",
            "Epoch 427/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0311 - val_loss: 1.7964\n",
            "Epoch 428/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0307 - val_loss: 1.7902\n",
            "Epoch 429/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0312 - val_loss: 1.7875\n",
            "Epoch 430/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0307 - val_loss: 1.7915\n",
            "Epoch 431/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0300 - val_loss: 1.8056\n",
            "Epoch 432/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0325 - val_loss: 1.7836\n",
            "Epoch 433/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0296 - val_loss: 1.7824\n",
            "Epoch 434/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0307 - val_loss: 1.8038\n",
            "Epoch 435/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0303 - val_loss: 1.7888\n",
            "Epoch 436/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0299 - val_loss: 1.7963\n",
            "Epoch 437/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0301 - val_loss: 1.7887\n",
            "Epoch 438/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0303 - val_loss: 1.7840\n",
            "Epoch 439/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0296 - val_loss: 1.8022\n",
            "Epoch 440/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0297 - val_loss: 1.8039\n",
            "Epoch 441/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0294 - val_loss: 1.7929\n",
            "Epoch 442/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0289 - val_loss: 1.8063\n",
            "Epoch 443/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0292 - val_loss: 1.7965\n",
            "Epoch 444/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0292 - val_loss: 1.7998\n",
            "Epoch 445/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0288 - val_loss: 1.7889\n",
            "Epoch 446/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0295 - val_loss: 1.7991\n",
            "Epoch 447/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0285 - val_loss: 1.8107\n",
            "Epoch 448/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0284 - val_loss: 1.8157\n",
            "Epoch 449/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0285 - val_loss: 1.8162\n",
            "Epoch 450/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0288 - val_loss: 1.8122\n",
            "Epoch 451/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0287 - val_loss: 1.8198\n",
            "Epoch 452/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0283 - val_loss: 1.7966\n",
            "Epoch 453/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0280 - val_loss: 1.8119\n",
            "Epoch 454/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0292 - val_loss: 1.8014\n",
            "Epoch 455/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0275 - val_loss: 1.8169\n",
            "Epoch 456/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0282 - val_loss: 1.8128\n",
            "Epoch 457/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0281 - val_loss: 1.8068\n",
            "Epoch 458/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0277 - val_loss: 1.8255\n",
            "Epoch 459/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0279 - val_loss: 1.8186\n",
            "Epoch 460/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0271 - val_loss: 1.8320\n",
            "Epoch 461/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0278 - val_loss: 1.8308\n",
            "Epoch 462/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0269 - val_loss: 1.8286\n",
            "Epoch 463/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0268 - val_loss: 1.8302\n",
            "Epoch 464/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0274 - val_loss: 1.8260\n",
            "Epoch 465/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0270 - val_loss: 1.8334\n",
            "Epoch 466/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0272 - val_loss: 1.8311\n",
            "Epoch 467/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0262 - val_loss: 1.8307\n",
            "Epoch 468/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0267 - val_loss: 1.8335\n",
            "Epoch 469/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0269 - val_loss: 1.8368\n",
            "Epoch 470/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0266 - val_loss: 1.8246\n",
            "Epoch 471/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0281 - val_loss: 1.8370\n",
            "Epoch 472/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0282 - val_loss: 1.8243\n",
            "Epoch 473/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0275 - val_loss: 1.8288\n",
            "Epoch 474/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0275 - val_loss: 1.8319\n",
            "Epoch 475/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0280 - val_loss: 1.8324\n",
            "Epoch 476/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0270 - val_loss: 1.8316\n",
            "Epoch 477/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0272 - val_loss: 1.8308\n",
            "Epoch 478/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0264 - val_loss: 1.8284\n",
            "Epoch 479/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0268 - val_loss: 1.8382\n",
            "Epoch 480/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0261 - val_loss: 1.8388\n",
            "Epoch 481/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0250 - val_loss: 1.8347\n",
            "Epoch 482/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0260 - val_loss: 1.8342\n",
            "Epoch 483/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0257 - val_loss: 1.8335\n",
            "Epoch 484/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0257 - val_loss: 1.8340\n",
            "Epoch 485/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0254 - val_loss: 1.8309\n",
            "Epoch 486/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0257 - val_loss: 1.8531\n",
            "Epoch 487/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0254 - val_loss: 1.8465\n",
            "Epoch 488/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0252 - val_loss: 1.8438\n",
            "Epoch 489/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0251 - val_loss: 1.8391\n",
            "Epoch 490/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0251 - val_loss: 1.8421\n",
            "Epoch 491/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0255 - val_loss: 1.8444\n",
            "Epoch 492/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0242 - val_loss: 1.8400\n",
            "Epoch 493/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0256 - val_loss: 1.8426\n",
            "Epoch 494/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0244 - val_loss: 1.8636\n",
            "Epoch 495/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0243 - val_loss: 1.8437\n",
            "Epoch 496/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0239 - val_loss: 1.8500\n",
            "Epoch 497/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0248 - val_loss: 1.8579\n",
            "Epoch 498/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0246 - val_loss: 1.8524\n",
            "Epoch 499/500\n",
            "2222/2222 [==============================] - 10s 5ms/step - loss: 0.0238 - val_loss: 1.8448\n",
            "Epoch 500/500\n",
            "2222/2222 [==============================] - 10s 4ms/step - loss: 0.0236 - val_loss: 1.8604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Veg3zTORUulo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Next: inference mode (sampling).\n",
        "# Here's the drill:\n",
        "# 1) encode input and retrieve initial decoder state\n",
        "# 2) run one step of decoder with this initial state\n",
        "# and a \"start of sequence\" token as target.\n",
        "# Output will be the next target token\n",
        "# 3) Repeat with the current target token and current states\n",
        "\n",
        "# Define sampling models\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nlv28XbhUulq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Nk4NYJXUuls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzXjFeHdUulu",
        "colab_type": "code",
        "outputId": "5feeebe5-413b-4a19-9d97-40d8cf3501f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6817
        }
      },
      "source": [
        "for seq_index in range(100):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: टॉम मेरा लड़का है।\n",
            "\n",
            "-\n",
            "Input sentence: Help!\n",
            "Decoded sentence: यह लो, तुम्हारा बस्ता।\n",
            "\n",
            "-\n",
            "Input sentence: Jump.\n",
            "Decoded sentence: उछलो.\n",
            "\n",
            "-\n",
            "Input sentence: Jump.\n",
            "Decoded sentence: उछलो.\n",
            "\n",
            "-\n",
            "Input sentence: Jump.\n",
            "Decoded sentence: उछलो.\n",
            "\n",
            "-\n",
            "Input sentence: Hello!\n",
            "Decoded sentence: नमस्कार।\n",
            "\n",
            "-\n",
            "Input sentence: Hello!\n",
            "Decoded sentence: नमस्कार।\n",
            "\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: चीन जापान से काफ़ी ज़्यादा बड़ा है।\n",
            "\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: चीन जापान से काफ़ी ज़्यादा बड़ा है।\n",
            "\n",
            "-\n",
            "Input sentence: Got it?\n",
            "Decoded sentence: समझे कि नहीं?\n",
            "\n",
            "-\n",
            "Input sentence: I'm OK.\n",
            "Decoded sentence: मैं डॉक्टर हूँ।\n",
            "\n",
            "-\n",
            "Input sentence: Awesome!\n",
            "Decoded sentence: बहुत बढ़िया!\n",
            "\n",
            "-\n",
            "Input sentence: Come in.\n",
            "Decoded sentence: लड़के तो लड़के ही रहेंगे।\n",
            "\n",
            "-\n",
            "Input sentence: Get out!\n",
            "Decoded sentence: बाहर निकल जाओ!\n",
            "\n",
            "-\n",
            "Input sentence: Go away!\n",
            "Decoded sentence: चले जाओ!\n",
            "\n",
            "-\n",
            "Input sentence: Goodbye!\n",
            "Decoded sentence: ख़ुदा हाफ़िज़।\n",
            "\n",
            "-\n",
            "Input sentence: Perfect!\n",
            "Decoded sentence: सही!\n",
            "\n",
            "-\n",
            "Input sentence: Perfect!\n",
            "Decoded sentence: सही!\n",
            "\n",
            "-\n",
            "Input sentence: Welcome.\n",
            "Decoded sentence: आपका स्वागत है।\n",
            "\n",
            "-\n",
            "Input sentence: Welcome.\n",
            "Decoded sentence: आपका स्वागत है।\n",
            "\n",
            "-\n",
            "Input sentence: Have fun.\n",
            "Decoded sentence: तुम कभी अफ़्रीका गए हुए हो क्या?\n",
            "\n",
            "-\n",
            "Input sentence: Have fun.\n",
            "Decoded sentence: तुम कभी अफ़्रीका गए हुए हो क्या?\n",
            "\n",
            "-\n",
            "Input sentence: Have fun.\n",
            "Decoded sentence: तुम कभी अफ़्रीका गए हुए हो क्या?\n",
            "\n",
            "-\n",
            "Input sentence: I forgot.\n",
            "Decoded sentence: मैं भूल गया।\n",
            "\n",
            "-\n",
            "Input sentence: I forgot.\n",
            "Decoded sentence: मैं भूल गया।\n",
            "\n",
            "-\n",
            "Input sentence: I'll pay.\n",
            "Decoded sentence: मैं हिल नहीं सकता।\n",
            "\n",
            "-\n",
            "Input sentence: I'm fine.\n",
            "Decoded sentence: मैं ठीक हूँ।\n",
            "\n",
            "-\n",
            "Input sentence: I'm full.\n",
            "Decoded sentence: माफ़ कीजिएगा, पर मैं आपको  क्या करता है।\n",
            "\n",
            "-\n",
            "Input sentence: Let's go!\n",
            "Decoded sentence: चलो चलें!\n",
            "\n",
            "-\n",
            "Input sentence: Answer me.\n",
            "Decoded sentence: मुझे जवाब दो।\n",
            "\n",
            "-\n",
            "Input sentence: Birds fly.\n",
            "Decoded sentence: पंछी उड़ते हैं।\n",
            "\n",
            "-\n",
            "Input sentence: Excuse me.\n",
            "Decoded sentence: माफ़ कीजिएगा, यहाँ कोई बैठा हुआ है क्या?\n",
            "\n",
            "-\n",
            "Input sentence: Fantastic!\n",
            "Decoded sentence: पागल मत बनो।\n",
            "\n",
            "-\n",
            "Input sentence: I fainted.\n",
            "Decoded sentence: मैं बेहोशी में चला गया।\n",
            "\n",
            "-\n",
            "Input sentence: I fear so.\n",
            "Decoded sentence: मुझे उससे प्यार हो गया।\n",
            "\n",
            "-\n",
            "Input sentence: I laughed.\n",
            "Decoded sentence: मैंने उससे बैठ जाने का इशारा किया।\n",
            "\n",
            "-\n",
            "Input sentence: I'm bored.\n",
            "Decoded sentence: मैं बोर हो रहा हूँ।\n",
            "\n",
            "-\n",
            "Input sentence: I'm broke.\n",
            "Decoded sentence: मैं बोर हो रहा हूँ।\n",
            "\n",
            "-\n",
            "Input sentence: I'm tired.\n",
            "Decoded sentence: मैं एक बूढ़े आदमी की तलाश मे हूँ।\n",
            "\n",
            "-\n",
            "Input sentence: It's cold.\n",
            "Decoded sentence: बरफ़ गिरनी शुरू गो गई।\n",
            "\n",
            "-\n",
            "Input sentence: Well done!\n",
            "Decoded sentence: मैं तुम्हारी पेनसिल इस्तेमाल कर सकता हूँ क्या?\n",
            "\n",
            "-\n",
            "Input sentence: Who knows?\n",
            "Decoded sentence: किसे पता है?\n",
            "\n",
            "-\n",
            "Input sentence: Who knows?\n",
            "Decoded sentence: किसे पता है?\n",
            "\n",
            "-\n",
            "Input sentence: Who knows?\n",
            "Decoded sentence: किसे पता है?\n",
            "\n",
            "-\n",
            "Input sentence: Who knows?\n",
            "Decoded sentence: किसे पता है?\n",
            "\n",
            "-\n",
            "Input sentence: Wonderful!\n",
            "Decoded sentence: क्या मैं कमरा साफ़ कर सकती हूँ?\n",
            "\n",
            "-\n",
            "Input sentence: Birds sing.\n",
            "Decoded sentence: पंछी गाते हैं।\n",
            "\n",
            "-\n",
            "Input sentence: Come on in.\n",
            "Decoded sentence: अंदर आ जाओ।\n",
            "\n",
            "-\n",
            "Input sentence: Definitely!\n",
            "Decoded sentence: टीवी चालू कर दोगे क्या?\n",
            "\n",
            "-\n",
            "Input sentence: Don't move.\n",
            "Decoded sentence: हिलो मत।\n",
            "\n",
            "-\n",
            "Input sentence: Fire burns.\n",
            "Decoded sentence: मैं तुम्हारी पेनसिल इस्तेमाल कर सकता हूँ क्या?\n",
            "\n",
            "-\n",
            "Input sentence: Follow him.\n",
            "Decoded sentence: आओ हमारे साथ बैठो।\n",
            "\n",
            "-\n",
            "Input sentence: I can swim.\n",
            "Decoded sentence: मैं ट्रैफ़िक जैम में फँस गया था।\n",
            "\n",
            "-\n",
            "Input sentence: I can swim.\n",
            "Decoded sentence: मैं ट्रैफ़िक जैम में फँस गया था।\n",
            "\n",
            "-\n",
            "Input sentence: I love you.\n",
            "Decoded sentence: मैं भी बॉस्टन में ही रहता हूँ।\n",
            "\n",
            "-\n",
            "Input sentence: I love you.\n",
            "Decoded sentence: मैं भी बॉस्टन में ही रहता हूँ।\n",
            "\n",
            "-\n",
            "Input sentence: I love you.\n",
            "Decoded sentence: मैं भी बॉस्टन में ही रहता हूँ।\n",
            "\n",
            "-\n",
            "Input sentence: I love you.\n",
            "Decoded sentence: मैं भी बॉस्टन में ही रहता हूँ।\n",
            "\n",
            "-\n",
            "Input sentence: I love you.\n",
            "Decoded sentence: मैं भी बॉस्टन में ही रहता हूँ।\n",
            "\n",
            "-\n",
            "Input sentence: I will try.\n",
            "Decoded sentence: मैं नौ बजे से पहले वापस आ जाऊँगा।\n",
            "\n",
            "-\n",
            "Input sentence: I'm coming.\n",
            "Decoded sentence: मैं गाड़ी चला सकता हूँ।\n",
            "\n",
            "-\n",
            "Input sentence: I'm hungry!\n",
            "Decoded sentence: मुझे अम्रीकी साहित्य में दिलचस्पी है।\n",
            "\n",
            "-\n",
            "Input sentence: I'm hungry!\n",
            "Decoded sentence: मुझे अम्रीकी साहित्य में दिलचस्पी है।\n",
            "\n",
            "-\n",
            "Input sentence: Let him in.\n",
            "Decoded sentence: उसे अंदर भेजो।\n",
            "\n",
            "-\n",
            "Input sentence: Let him in.\n",
            "Decoded sentence: उसे अंदर भेजो।\n",
            "\n",
            "-\n",
            "Input sentence: Let me out!\n",
            "Decoded sentence: मुझे ट्राए करने दो।\n",
            "\n",
            "-\n",
            "Input sentence: Once again.\n",
            "Decoded sentence: हराऊएक मुंकर मुझक्या लोगीं कर सकते हो?\n",
            "\n",
            "-\n",
            "Input sentence: Please sit.\n",
            "Decoded sentence: मुझे एक कप चाय दीजिए।\n",
            "\n",
            "-\n",
            "Input sentence: That a boy!\n",
            "Decoded sentence: यह जूते उसके हैं।\n",
            "\n",
            "-\n",
            "Input sentence: What's new?\n",
            "Decoded sentence: तापमान क्या है?\n",
            "\n",
            "-\n",
            "Input sentence: What's new?\n",
            "Decoded sentence: तापमान क्या है?\n",
            "\n",
            "-\n",
            "Input sentence: Who's that?\n",
            "Decoded sentence: यह क्या है?\n",
            "\n",
            "-\n",
            "Input sentence: Don't shout.\n",
            "Decoded sentence: बहुत ख़ूब!\n",
            "\n",
            "-\n",
            "Input sentence: Don't shout.\n",
            "Decoded sentence: बहुत ख़ूब!\n",
            "\n",
            "-\n",
            "Input sentence: He stood up.\n",
            "Decoded sentence: वह मेरा अच्छा दोस्त था।\n",
            "\n",
            "-\n",
            "Input sentence: He's strong.\n",
            "Decoded sentence: वह ताकतवर है।\n",
            "\n",
            "-\n",
            "Input sentence: How are you?\n",
            "Decoded sentence: आप कैसे हो?\n",
            "\n",
            "-\n",
            "Input sentence: How are you?\n",
            "Decoded sentence: आप कैसे हो?\n",
            "\n",
            "-\n",
            "Input sentence: How are you?\n",
            "Decoded sentence: आप कैसे हो?\n",
            "\n",
            "-\n",
            "Input sentence: How are you?\n",
            "Decoded sentence: आप कैसे हो?\n",
            "\n",
            "-\n",
            "Input sentence: How are you?\n",
            "Decoded sentence: आप कैसे हो?\n",
            "\n",
            "-\n",
            "Input sentence: How are you?\n",
            "Decoded sentence: आप कैसे हो?\n",
            "\n",
            "-\n",
            "Input sentence: How are you?\n",
            "Decoded sentence: आप कैसे हो?\n",
            "\n",
            "-\n",
            "Input sentence: I like both.\n",
            "Decoded sentence: मुझे विज्ञान और गणित दोनों पसंद हैं।\n",
            "\n",
            "-\n",
            "Input sentence: I like cake.\n",
            "Decoded sentence: मुझे नौवे महीने का नाम बताओ।\n",
            "\n",
            "-\n",
            "Input sentence: I like dogs.\n",
            "Decoded sentence: मुझे कुत्ते अच्छे लगते हैं।\n",
            "\n",
            "-\n",
            "Input sentence: I like math.\n",
            "Decoded sentence: मुझे उसका पता पता है।\n",
            "\n",
            "-\n",
            "Input sentence: I'll attend.\n",
            "Decoded sentence: मैं शार शाया होता हूँ।\n",
            "\n",
            "-\n",
            "Input sentence: Nobody came.\n",
            "Decoded sentence: बहुत ख़ूब!\n",
            "\n",
            "-\n",
            "Input sentence: Was I wrong?\n",
            "Decoded sentence: क्या मैं ग़लत था?\n",
            "\n",
            "-\n",
            "Input sentence: What's this?\n",
            "Decoded sentence: निश्चित ही\n",
            "\n",
            "-\n",
            "Input sentence: Are you sick?\n",
            "Decoded sentence: क्या तुम्हे मुझपर यकीन है?\n",
            "\n",
            "-\n",
            "Input sentence: Bring him in.\n",
            "Decoded sentence: मुबारक हो!\n",
            "\n",
            "-\n",
            "Input sentence: Come with us.\n",
            "Decoded sentence: जल्दी घर आजाओ।\n",
            "\n",
            "-\n",
            "Input sentence: Happy Easter!\n",
            "Decoded sentence: आप कैसे हो?\n",
            "\n",
            "-\n",
            "Input sentence: Has Tom left?\n",
            "Decoded sentence: इसे जल्द-से-जल्द ख़त्म करो ।\n",
            "\n",
            "-\n",
            "Input sentence: I am at home.\n",
            "Decoded sentence: मैं नतीजे से खुश हूँ।\n",
            "\n",
            "-\n",
            "Input sentence: I can't move.\n",
            "Decoded sentence: मैं ट्रैफ़िक जैम में फँस गया था।\n",
            "\n",
            "-\n",
            "Input sentence: I don't know.\n",
            "Decoded sentence: मुझे नहीं पता।\n",
            "\n",
            "-\n",
            "Input sentence: I don't know.\n",
            "Decoded sentence: मुझे नहीं पता।\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIxUj6NO4KLW",
        "colab_type": "text"
      },
      "source": [
        "#Summary:\n",
        "### From the above, we can say that most of the sentences are being translated correctly."
      ]
    }
  ]
}